{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "def25a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c844223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hitesh\\AppData\\Local\\Temp\\ipykernel_11608\\4130650133.py:17: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data[asset] = yf.download(ticker, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\Hitesh\\AppData\\Local\\Temp\\ipykernel_11608\\4130650133.py:17: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data[asset] = yf.download(ticker, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\Hitesh\\AppData\\Local\\Temp\\ipykernel_11608\\4130650133.py:17: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data[asset] = yf.download(ticker, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\Hitesh\\AppData\\Local\\Temp\\ipykernel_11608\\4130650133.py:17: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data[asset] = yf.download(ticker, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\Hitesh\\AppData\\Local\\Temp\\ipykernel_11608\\4130650133.py:17: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data[asset] = yf.download(ticker, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Define the tickers for the financial assets\n",
    "tickers = {\n",
    "    'S&P 500': '^GSPC',\n",
    "    'FTSE 100': '^FTSE',\n",
    "    'Nikkei 225': '^N225',\n",
    "    'Gold ETF': 'GLD',  \n",
    "    'US Treasury Bonds': 'TLT' \n",
    "}\n",
    "\n",
    "# Define the time period for the data\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2020-12-31'\n",
    "\n",
    "# Download data for each ticker\n",
    "data = {}\n",
    "for asset, ticker in tickers.items():\n",
    "    data[asset] = yf.download(ticker, start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9faee916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S&P 500': Price             Close         High          Low         Open      Volume\n",
       " Ticker            ^GSPC        ^GSPC        ^GSPC        ^GSPC       ^GSPC\n",
       " Date                                                                      \n",
       " 2010-01-04  1132.989990  1133.869995  1116.560059  1116.560059  3991400000\n",
       " 2010-01-05  1136.520020  1136.630005  1129.660034  1132.660034  2491020000\n",
       " 2010-01-06  1137.140015  1139.189941  1133.949951  1135.709961  4972660000\n",
       " 2010-01-07  1141.689941  1142.459961  1131.319946  1136.270020  5270680000\n",
       " 2010-01-08  1144.979980  1145.390015  1136.219971  1140.520020  4389590000\n",
       " ...                 ...          ...          ...          ...         ...\n",
       " 2020-12-23  3690.010010  3711.239990  3689.280029  3693.419922  3779160000\n",
       " 2020-12-24  3703.060059  3703.820068  3689.320068  3694.030029  1883780000\n",
       " 2020-12-28  3735.360107  3740.510010  3723.030029  3723.030029  3535460000\n",
       " 2020-12-29  3727.040039  3756.120117  3723.310059  3750.010010  3393290000\n",
       " 2020-12-30  3732.040039  3744.629883  3730.209961  3736.189941  3154850000\n",
       " \n",
       " [2768 rows x 5 columns],\n",
       " 'FTSE 100': Price             Close         High          Low         Open      Volume\n",
       " Ticker            ^FTSE        ^FTSE        ^FTSE        ^FTSE       ^FTSE\n",
       " Date                                                                      \n",
       " 2010-01-04  5500.299805  5500.299805  5410.799805  5412.899902   750942000\n",
       " 2010-01-05  5522.500000  5536.399902  5480.700195  5500.299805  1149301200\n",
       " 2010-01-06  5530.000000  5536.500000  5497.700195  5522.500000   998295300\n",
       " 2010-01-07  5526.700195  5551.700195  5499.799805  5530.000000  1162933700\n",
       " 2010-01-08  5534.200195  5549.299805  5494.799805  5526.700195  1006420600\n",
       " ...                 ...          ...          ...          ...         ...\n",
       " 2020-12-21  6416.299805  6529.200195  6315.299805  6529.200195  1118314800\n",
       " 2020-12-23  6495.799805  6497.299805  6431.200195  6453.200195   807695400\n",
       " 2020-12-24  6502.100098  6530.299805  6479.600098  6495.799805   477470700\n",
       " 2020-12-29  6602.700195  6676.600098  6502.100098  6502.100098   824116800\n",
       " 2020-12-30  6555.799805  6623.000000  6552.500000  6602.700195   425493000\n",
       " \n",
       " [2777 rows x 5 columns],\n",
       " 'Nikkei 225': Price              Close          High           Low          Open     Volume\n",
       " Ticker             ^N225         ^N225         ^N225         ^N225      ^N225\n",
       " Date                                                                         \n",
       " 2010-01-04  10654.790039  10694.490234  10608.139648  10609.339844  104400000\n",
       " 2010-01-05  10681.830078  10791.040039  10655.570312  10719.440430  166200000\n",
       " 2010-01-06  10731.450195  10768.610352  10661.169922  10709.549805  181800000\n",
       " 2010-01-07  10681.660156  10774.000000  10636.669922  10742.750000  182600000\n",
       " 2010-01-08  10798.320312  10816.450195  10677.559570  10743.299805  211800000\n",
       " ...                  ...           ...           ...           ...        ...\n",
       " 2020-12-24  26668.349609  26764.529297  26605.259766  26635.109375   47900000\n",
       " 2020-12-25  26656.609375  26716.609375  26638.279297  26708.099609   33400000\n",
       " 2020-12-28  26854.029297  26854.029297  26664.599609  26691.289062   50700000\n",
       " 2020-12-29  27568.150391  27602.519531  26921.140625  26936.380859   59400000\n",
       " 2020-12-30  27444.169922  27572.570312  27338.560547  27559.099609   50600000\n",
       " \n",
       " [2690 rows x 5 columns],\n",
       " 'Gold ETF': Price            Close        High         Low        Open    Volume\n",
       " Ticker             GLD         GLD         GLD         GLD       GLD\n",
       " Date                                                                \n",
       " 2010-01-04  109.800003  110.139999  109.309998  109.820000  16224100\n",
       " 2010-01-05  109.699997  110.389999  109.260002  109.879997  14213100\n",
       " 2010-01-06  111.510002  111.769997  110.410004  110.709999  24981900\n",
       " 2010-01-07  110.820000  111.290001  110.620003  111.070000  13609800\n",
       " 2010-01-08  111.370003  111.580002  110.260002  111.519997  15894600\n",
       " ...                ...         ...         ...         ...       ...\n",
       " 2020-12-23  175.649994  176.210007  175.059998  175.100006   6542800\n",
       " 2020-12-24  176.350006  176.369995  175.509995  175.550003   3695400\n",
       " 2020-12-28  175.710007  177.910004  175.630005  177.259995   7778700\n",
       " 2020-12-29  176.350006  176.970001  175.570007  176.250000   5983700\n",
       " 2020-12-30  177.699997  177.720001  176.440002  176.440002   5914000\n",
       " \n",
       " [2768 rows x 5 columns],\n",
       " 'US Treasury Bonds': Price            Close        High         Low        Open   Volume\n",
       " Ticker             TLT         TLT         TLT         TLT      TLT\n",
       " Date                                                               \n",
       " 2010-01-04   58.250439   58.438532   58.101264   58.269896  2829100\n",
       " 2010-01-05   58.626617   58.782279   58.373665   58.406097  2841600\n",
       " 2010-01-06   57.841805   58.542290   57.802890   58.483913  4099600\n",
       " 2010-01-07   57.939087   58.140150   57.802882   57.867741  2793200\n",
       " 2010-01-08   57.913139   58.088258   57.569384   58.055831  2910700\n",
       " ...                ...         ...         ...         ...      ...\n",
       " 2020-12-23  137.479904  137.497458  136.418121  137.436026  9266600\n",
       " 2020-12-24  138.023956  138.146808  137.637862  137.681727  3117100\n",
       " 2020-12-28  138.085312  138.164285  136.979651  137.225354  7791100\n",
       " 2020-12-29  137.909897  138.023964  137.207884  137.234208  9030900\n",
       " 2020-12-30  138.208237  138.234561  137.488670  137.611535  9097300\n",
       " \n",
       " [2768 rows x 5 columns]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e56218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price             Close         High          Low         Open      Volume  \\\n",
      "Ticker            ^GSPC        ^GSPC        ^GSPC        ^GSPC       ^GSPC   \n",
      "Date                                                                         \n",
      "2020-12-23  3690.010010  3711.239990  3689.280029  3693.419922  3779160000   \n",
      "2020-12-24  3703.060059  3703.820068  3689.320068  3694.030029  1883780000   \n",
      "2020-12-28  3735.360107  3740.510010  3723.030029  3723.030029  3535460000   \n",
      "2020-12-29  3727.040039  3756.120117  3723.310059  3750.010010  3393290000   \n",
      "2020-12-30  3732.040039  3744.629883  3730.209961  3736.189941  3154850000   \n",
      "\n",
      "Price      Log Return  \n",
      "Ticker                 \n",
      "Date                   \n",
      "2020-12-23   0.000746  \n",
      "2020-12-24   0.003530  \n",
      "2020-12-28   0.008685  \n",
      "2020-12-29  -0.002230  \n",
      "2020-12-30   0.001341  \n"
     ]
    }
   ],
   "source": [
    "# Function to calculate log returns\n",
    "def calculate_log_returns(df):\n",
    "    df['Log Return'] = df['Close'].pct_change().apply(lambda x: np.log(1 + x))\n",
    "    return df\n",
    "\n",
    "# Calculate log returns for each asset\n",
    "for asset in data:\n",
    "    data[asset] = calculate_log_returns(data[asset])\n",
    "\n",
    "print(data['S&P 500'].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c615f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price             Close         High          Low         Open      Volume  \\\n",
      "Ticker            ^GSPC        ^GSPC        ^GSPC        ^GSPC       ^GSPC   \n",
      "Date                                                                         \n",
      "2020-12-23  3690.010010  3711.239990  3689.280029  3693.419922  3779160000   \n",
      "2020-12-24  3703.060059  3703.820068  3689.320068  3694.030029  1883780000   \n",
      "2020-12-28  3735.360107  3740.510010  3723.030029  3723.030029  3535460000   \n",
      "2020-12-29  3727.040039  3756.120117  3723.310059  3750.010010  3393290000   \n",
      "2020-12-30  3732.040039  3744.629883  3730.209961  3736.189941  3154850000   \n",
      "\n",
      "Price      Log Return     5-day MA    21-day MA  \n",
      "Ticker                                           \n",
      "Date                                             \n",
      "2020-12-23   0.000746  3700.815967  3674.680466  \n",
      "2020-12-24   0.003530  3696.931982  3677.901902  \n",
      "2020-12-28   0.008685  3702.122021  3682.935721  \n",
      "2020-12-29  -0.002230  3708.546045  3687.159052  \n",
      "2020-12-30   0.001341  3717.502051  3692.416678  \n"
     ]
    }
   ],
   "source": [
    "# Add 5-day and 21-day moving averages for each asset\n",
    "for asset, df in data.items():\n",
    "    df['5-day MA'] = df['Close'].rolling(window=5).mean()\n",
    "    df['21-day MA'] = df['Close'].rolling(window=21).mean()\n",
    "\n",
    "\n",
    "print(data['S&P 500'].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9cad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price             Close         High          Low         Open      Volume  \\\n",
      "Ticker            ^GSPC        ^GSPC        ^GSPC        ^GSPC       ^GSPC   \n",
      "Date                                                                         \n",
      "2020-12-23  3690.010010  3711.239990  3689.280029  3693.419922  3779160000   \n",
      "2020-12-24  3703.060059  3703.820068  3689.320068  3694.030029  1883780000   \n",
      "2020-12-28  3735.360107  3740.510010  3723.030029  3723.030029  3535460000   \n",
      "2020-12-29  3727.040039  3756.120117  3723.310059  3750.010010  3393290000   \n",
      "2020-12-30  3732.040039  3744.629883  3730.209961  3736.189941  3154850000   \n",
      "\n",
      "Price      Log Return     5-day MA    21-day MA Volatility  \n",
      "Ticker                                                      \n",
      "Date                                                        \n",
      "2020-12-23   0.000746  3700.815967  3674.680466   0.006239  \n",
      "2020-12-24   0.003530  3696.931982  3677.901902   0.005307  \n",
      "2020-12-28   0.008685  3702.122021  3682.935721   0.005537  \n",
      "2020-12-29  -0.002230  3708.546045  3687.159052   0.005586  \n",
      "2020-12-30   0.001341  3717.502051  3692.416678   0.005428  \n"
     ]
    }
   ],
   "source": [
    "# Add 21-day rolling volatility for each asset\n",
    "for asset, df in data.items():\n",
    "    df['Volatility'] = df['Log Return'].rolling(window=21).std()\n",
    "\n",
    "\n",
    "print(data['S&P 500'].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7baf1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price             Close         High          Low         Open      Volume  \\\n",
      "Ticker            ^GSPC        ^GSPC        ^GSPC        ^GSPC       ^GSPC   \n",
      "Date                                                                         \n",
      "2020-12-23  3690.010010  3711.239990  3689.280029  3693.419922  3779160000   \n",
      "2020-12-24  3703.060059  3703.820068  3689.320068  3694.030029  1883780000   \n",
      "2020-12-28  3735.360107  3740.510010  3723.030029  3723.030029  3535460000   \n",
      "2020-12-29  3727.040039  3756.120117  3723.310059  3750.010010  3393290000   \n",
      "2020-12-30  3732.040039  3744.629883  3730.209961  3736.189941  3154850000   \n",
      "\n",
      "Price      Log Return     5-day MA    21-day MA Volatility  \n",
      "Ticker                                                      \n",
      "Date                                                        \n",
      "2020-12-23   0.000746  3700.815967  3674.680466   0.006239  \n",
      "2020-12-24   0.003530  3696.931982  3677.901902   0.005307  \n",
      "2020-12-28   0.008685  3702.122021  3682.935721   0.005537  \n",
      "2020-12-29  -0.002230  3708.546045  3687.159052   0.005586  \n",
      "2020-12-30   0.001341  3717.502051  3692.416678   0.005428  \n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values \n",
    "for asset in data:\n",
    "    data[asset].dropna(inplace=True)\n",
    "\n",
    "\n",
    "print(data['S&P 500'].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9fd603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            S&P 500 Log Return  S&P 500 5-day MA  S&P 500 21-day MA  \\\n",
      "Date                                                                  \n",
      "2020-12-24            0.003530       3696.931982        3677.901902   \n",
      "2020-12-25                 NaN               NaN                NaN   \n",
      "2020-12-28            0.008685       3702.122021        3682.935721   \n",
      "2020-12-29           -0.002230       3708.546045        3687.159052   \n",
      "2020-12-30            0.001341       3717.502051        3692.416678   \n",
      "\n",
      "            S&P 500 Volatility  FTSE 100 Log Return  FTSE 100 5-day MA  \\\n",
      "Date                                                                     \n",
      "2020-12-24            0.005307             0.000969        6498.900000   \n",
      "2020-12-25                 NaN                  NaN                NaN   \n",
      "2020-12-28            0.005537                  NaN                NaN   \n",
      "2020-12-29            0.005586             0.015353        6509.220020   \n",
      "2020-12-30            0.005428            -0.007129        6514.539941   \n",
      "\n",
      "            FTSE 100 21-day MA  FTSE 100 Volatility  Nikkei 225 Log Return  \\\n",
      "Date                                                                         \n",
      "2020-12-24         6486.285668             0.008887               0.005398   \n",
      "2020-12-25                 NaN                  NaN              -0.000440   \n",
      "2020-12-28                 NaN                  NaN               0.007379   \n",
      "2020-12-29         6496.361863             0.009299               0.026245   \n",
      "2020-12-30         6505.547573             0.009404              -0.004507   \n",
      "\n",
      "            Nikkei 225 5-day MA  Nikkei 225 21-day MA  Nikkei 225 Volatility  \\\n",
      "Date                                                                           \n",
      "2020-12-24         26621.467969          26671.317987               0.006221   \n",
      "2020-12-25         26600.111719          26676.998884               0.005916   \n",
      "2020-12-28         26628.033594          26686.966425               0.006067   \n",
      "2020-12-29         26854.385547          26740.991722               0.008001   \n",
      "2020-12-30         27038.261719          26772.259859               0.007681   \n",
      "\n",
      "            Gold ETF Log Return  Gold ETF 5-day MA  Gold ETF 21-day MA  \\\n",
      "Date                                                                     \n",
      "2020-12-24             0.003977         175.762003          173.058095   \n",
      "2020-12-25                  NaN                NaN                 NaN   \n",
      "2020-12-28            -0.003636         175.616003          173.353334   \n",
      "2020-12-29             0.003636         175.710004          173.760954   \n",
      "2020-12-30             0.007626         176.352002          174.286192   \n",
      "\n",
      "            Gold ETF Volatility  US Treasury Bonds Log Return  \\\n",
      "Date                                                            \n",
      "2020-12-24             0.009141                      0.003950   \n",
      "2020-12-25                  NaN                           NaN   \n",
      "2020-12-28             0.009207                      0.000444   \n",
      "2020-12-29             0.008798                     -0.001271   \n",
      "2020-12-30             0.008613                      0.002161   \n",
      "\n",
      "            US Treasury Bonds 5-day MA  US Treasury Bonds 21-day MA  \\\n",
      "Date                                                                  \n",
      "2020-12-24                  137.769464                   138.123658   \n",
      "2020-12-25                         NaN                          NaN   \n",
      "2020-12-28                  137.950229                   138.082155   \n",
      "2020-12-29                  137.987085                   137.968079   \n",
      "2020-12-30                  137.941461                   137.876553   \n",
      "\n",
      "            US Treasury Bonds Volatility  \n",
      "Date                                      \n",
      "2020-12-24                      0.007137  \n",
      "2020-12-25                           NaN  \n",
      "2020-12-28                      0.007114  \n",
      "2020-12-29                      0.006739  \n",
      "2020-12-30                      0.006769  \n"
     ]
    }
   ],
   "source": [
    "# Combine data for all assets into a single DataFrame\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "for asset, df in data.items():\n",
    "    asset_data = df[['Log Return', '5-day MA', '21-day MA', 'Volatility']]\n",
    "    asset_data.columns = [f'{asset} Log Return', f'{asset} 5-day MA', f'{asset} 21-day MA', f'{asset} Volatility']\n",
    "    merged_data = pd.concat([merged_data, asset_data], axis=1)\n",
    "\n",
    "\n",
    "print(merged_data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932aa24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asset in data:\n",
    "    merged_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9df770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2028, 1, 20) (508, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Drop the last row from both the features and target to align their lengths\n",
    "features = merged_data.values[:-1]  # Drop the last row of features to match target length\n",
    "target = merged_data['S&P 500 Log Return'].shift(-1).dropna().values  # Shift target and drop NaN values\n",
    "\n",
    "# Ensure that both features and target arrays have the same length after dropping the last row\n",
    "assert len(features) == len(target), f\"Features and target length mismatch: {len(features)} != {len(target)}\"\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Reshape the data for LSTM input (samples, timesteps, features)\n",
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Check the shape of the data\n",
    "print(X_train_lstm.shape, X_test_lstm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ca53c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 2/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7163e-04 - val_loss: 0.0015\n",
      "Epoch 3/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7007e-04 - val_loss: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4634e-04 - val_loss: 8.3306e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5494e-04 - val_loss: 6.5558e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2904e-04 - val_loss: 6.3838e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2476e-04 - val_loss: 5.7146e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3518e-04 - val_loss: 6.0804e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2284e-04 - val_loss: 6.5335e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1504e-04 - val_loss: 5.8238e-04\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8156e-04 \n",
      "LSTM Model Loss: 0.0005823844694532454\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Build the LSTM model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "# Add LSTM layer (units = 50, return_sequences=False to output only the final time step)\n",
    "model_lstm.add(LSTM(units=50, return_sequences=False, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "\n",
    "# Add Dense layer for prediction\n",
    "model_lstm.add(Dense(units=1))  # Predicting a single value (next day's return)\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the LSTM model\n",
    "history_lstm = model_lstm.fit(X_train_lstm, y_train, epochs=10, batch_size=32, validation_data=(X_test_lstm, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "lstm_loss = model_lstm.evaluate(X_test_lstm, y_test)\n",
    "print(\"LSTM Model Loss:\", lstm_loss)\n",
    "\n",
    "y_pred_lstm = model_lstm.predict(X_test_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc9e2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2028, 20]) torch.Size([1, 508, 20])\n",
      "torch.Size([2028]) torch.Size([508])\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the input to match (seq_len, batch_size, input_dim)\n",
    "# Since you have a single timestep (sequence length = 1), you can reshape to (1, batch_size, input_dim)\n",
    "X_train_tensor = X_train_tensor.unsqueeze(0)  # Adding sequence length dimension (1)\n",
    "X_test_tensor = X_test_tensor.unsqueeze(0)  # Adding sequence length dimension (1)\n",
    "\n",
    "# Check the new shapes\n",
    "print(X_train_tensor.shape, X_test_tensor.shape)  # Should be (1, batch_size, input_dim)\n",
    "print(y_train_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8dd792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, output_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Transformer block (No target input, only source)\n",
    "        self.transformer = nn.Transformer(d_model=input_dim, nhead=num_heads, num_encoder_layers=num_layers)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, seq_len, input_dim)\n",
    "        x = x.permute(1, 0, 2)  # Change shape to (seq_len, batch_size, input_dim) for Transformer\n",
    "        \n",
    "        # Apply the Transformer\n",
    "        # Since we don't need a target (tgt), we will use the same input `x` as the input to the Transformer\n",
    "        x = self.transformer(x, x)  # Pass the same tensor as both source and target\n",
    "        \n",
    "        # Pooling across the sequence length (mean pooling)\n",
    "        x = x.mean(dim=0)  # Pooling across the sequence length\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model_transformer = TransformerModel(input_dim=X_train.shape[1], num_heads=4, num_layers=2, output_dim=1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_transformer.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35edc1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([2028, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.014851425774395466\n",
      "Epoch [2/10], Loss: 2.1736502647399902\n",
      "Epoch [3/10], Loss: 0.4221070110797882\n",
      "Epoch [4/10], Loss: 0.0003210810536984354\n",
      "Epoch [5/10], Loss: 0.11240652203559875\n",
      "Epoch [6/10], Loss: 0.15187445282936096\n",
      "Epoch [7/10], Loss: 0.09505989402532578\n",
      "Epoch [8/10], Loss: 0.03072057105600834\n",
      "Epoch [9/10], Loss: 0.0013898792676627636\n",
      "Epoch [10/10], Loss: 0.008257371373474598\n",
      "Transformer Model Loss: 0.04923277720808983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([508, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model_transformer.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model_transformer(X_train_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train_tensor.view(-1, 1))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Evaluate the Transformer model on test data\n",
    "model_transformer.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_transformer = model_transformer(X_test_tensor)\n",
    "    transformer_loss = criterion(y_pred_transformer, y_test_tensor.view(-1, 1))\n",
    "\n",
    "print(\"Transformer Model Loss:\", transformer_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40acfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM MSE: 0.0005823844981462914, R^2: -1.1372094351232964\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [508, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLSTM MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_lstm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R^2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_lstm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Transformer Evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m mse_transformer = \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_transformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m r2_transformer = r2_score(y_test, y_pred_transformer.numpy())\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTransformer MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_transformer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, R^2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_transformer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:580\u001b[39m, in \u001b[36mmean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[32m    531\u001b[39m \n\u001b[32m    532\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m \u001b[33;03m0.825...\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    578\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    579\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m )\n\u001b[32m    584\u001b[39m output_errors = _average((y_true - y_pred) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m, weights=sample_weight)\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m y_type, y_true, y_pred, sample_weight, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:114\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true, y_pred and sample_weight belong to the same regression task.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    116\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hitesh\\Downloads\\Class 9th\\Practice\\tf_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [508, 1]"
     ]
    }
   ],
   "source": [
    "# LSTM Evaluation\n",
    "mse_lstm = mean_squared_error(y_test, y_pred_lstm)\n",
    "r2_lstm = r2_score(y_test, y_pred_lstm)\n",
    "print(f\"LSTM MSE: {mse_lstm}, R^2: {r2_lstm}\")\n",
    "\n",
    "# Transformer Evaluation\n",
    "mse_transformer = mean_squared_error(y_test, y_pred_transformer.cpu().detach().numpy().flatten())\n",
    "r2_transformer = r2_score(y_test, y_pred_transformer.numpy())\n",
    "print(f\"Transformer MSE: {mse_transformer}, R^2: {r2_transformer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d53a7",
   "metadata": {},
   "source": [
    "Removing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634d40e",
   "metadata": {},
   "source": [
    "Method 1: With Hyperparameters  hidden_dim=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19c95b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # We only want the output of the last timestep\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255150dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.004320289473980665\n",
      "Epoch [20/50], Loss: 0.0005033991765230894\n",
      "Early stopping at epoch 24 with loss 0.0007177990046329796\n",
      "LSTM Model MSE: 0.003222272139579327, R²: -10.824954890049256\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "hidden_dim = 64  # Number of LSTM hidden units\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "dropout = 0.3  # Dropout rate to prevent overfitting\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "\n",
    "# Prepare the data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape for LSTM input (batch_size, seq_len, input_dim)\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1)  \n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = float('inf')\n",
    "patience = 5  # Number of epochs to wait before stopping if no improvement\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train_tensor.view(-1, 1))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Check if we should stop early\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1} with loss {loss.item()}\")\n",
    "        break\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get predictions\n",
    "    y_pred = model(X_test_tensor)\n",
    "    \n",
    "    # Convert predictions to NumPy\n",
    "    y_pred_numpy = y_pred.cpu().detach().numpy().flatten()\n",
    "    \n",
    "    # Calculate MSE and R²\n",
    "    mse = mean_squared_error(y_test, y_pred_numpy)\n",
    "    r2 = r2_score(y_test, y_pred_numpy)\n",
    "    \n",
    "    print(f\"LSTM Model MSE: {mse}, R²: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66288f0",
   "metadata": {},
   "source": [
    "Method 2: With Hyperparameters  hidden_dim=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daeba7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 9.962165495380759e-05\n",
      "Epoch [20/100], Loss: 0.00021199113689363003\n",
      "Early stopping at epoch 21 with loss 0.000183356532943435\n",
      "LSTM Model MSE: 0.00030985773410140605, R²: -0.13710250697830606, MAE: 0.012230074620605533\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_dim = 128  \n",
    "num_layers = 3    \n",
    "dropout = 0.2     \n",
    "learning_rate = 0.0005  \n",
    "epochs = 100  \n",
    "patience = 10  \n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1) \n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)    \n",
    "\n",
    "model = LSTMModel(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train_tensor.view(-1, 1))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Check if we should stop early\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1} with loss {loss.item()}\")\n",
    "        break\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the LSTM model on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_lstm = model(X_test_tensor)\n",
    "    \n",
    "    # Convert predictions to NumPy\n",
    "    y_pred_numpy = y_pred_lstm.cpu().detach().numpy().flatten()\n",
    "    \n",
    "    # Calculate MSE, R², and MAE\n",
    "    mse_lstm = mean_squared_error(y_test, y_pred_numpy)\n",
    "    r2_lstm = r2_score(y_test, y_pred_numpy)\n",
    "    mae_lstm = mean_absolute_error(y_test, y_pred_numpy)\n",
    "    \n",
    "    print(f\"LSTM Model MSE: {mse_lstm}, R²: {r2_lstm}, MAE: {mae_lstm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee2f48cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.00014450604794546962\n",
      "Early stopping at epoch 15 with loss 8.879851520759985e-05\n",
      "LSTM Model MSE: 0.0002683249700953755, R²: 0.015312633505293327, MAE: 0.009686178304511775\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for more complex model\n",
    "hidden_dim = 256  # Increased hidden units\n",
    "num_layers = 3    # Increased number of layers\n",
    "dropout = 0.1     # Moderate dropout rate\n",
    "learning_rate = 0.0003  # Reduced learning rate\n",
    "epochs = 100      # Increase epochs for more thorough training\n",
    "patience = 10     # Increased patience for early stopping\n",
    "\n",
    "# Prepare the data (assuming X_train_tensor and y_train_tensor are already defined)\n",
    "X_train_tensor = torch.tensor(X_train_lstm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_lstm, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model = LSTMModel(input_dim=X_train_lstm.shape[2], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_pred, y_train_tensor.view(-1, 1))\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Early stopping condition\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1} with loss {loss.item()}\")\n",
    "        break\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the LSTM model on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_lstm = model(X_test_tensor)\n",
    "    \n",
    "    # Convert predictions to NumPy\n",
    "    y_pred_numpy = y_pred_lstm.cpu().detach().numpy().flatten()\n",
    "    \n",
    "    # Calculate MSE, R², and MAE\n",
    "    mse_lstm = mean_squared_error(y_test, y_pred_numpy)\n",
    "    r2_lstm = r2_score(y_test, y_pred_numpy)\n",
    "    mae_lstm = mean_absolute_error(y_test, y_pred_numpy)\n",
    "    \n",
    "    print(f\"LSTM Model MSE: {mse_lstm}, R²: {r2_lstm}, MAE: {mae_lstm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae65e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.006269329693168402\n",
      "Epoch [20/50], Loss: 0.0034028906375169754\n",
      "Epoch [30/50], Loss: 0.001526078674942255\n",
      "Epoch [40/50], Loss: 0.0005605411133728921\n",
      "Epoch [50/50], Loss: 0.000251695018960163\n",
      "Test MSE: 0.002490169407748459, R²: -8.138315958331951, MAE: 0.040778098587653216\n",
      "Best model so far: MSE = 0.002490169407748459, Params = {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 9.723919356474653e-05\n",
      "Epoch [20/50], Loss: 0.00011713154526660219\n",
      "Early stopping at epoch 22 with loss 0.00011293414718238637\n",
      "Test MSE: 0.0002794740106351899, R²: -0.02560163311771335, MAE: 0.010053753588356118\n",
      "Best model so far: MSE = 0.0002794740106351899, Params = {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.0001286221668124199\n",
      "Epoch [20/50], Loss: 0.000158757422468625\n",
      "Early stopping at epoch 22 with loss 0.00014956246013753116\n",
      "Test MSE: 0.0002961833660920172, R²: -0.08692090286275023, MAE: 0.010876001552708427\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.00012592504208441824\n",
      "Early stopping at epoch 16 with loss 9.74909053184092e-05\n",
      "Test MSE: 0.0002738492507688451, R²: -0.004960132708759835, MAE: 0.009790750962258109\n",
      "Best model so far: MSE = 0.0002738492507688451, Params = {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.002344978041946888\n",
      "Epoch [20/50], Loss: 0.0005068080499768257\n",
      "Epoch [30/50], Loss: 0.0001637867244426161\n",
      "Early stopping at epoch 38 with loss 0.00018499714497011155\n",
      "Test MSE: 0.0009275108353001658, R²: -2.4037391357293076, MAE: 0.02261223368299219\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.00042986299376934767\n",
      "Epoch [20/50], Loss: 0.00010417487646918744\n",
      "Early stopping at epoch 28 with loss 0.000156196576426737\n",
      "Test MSE: 0.0002980960213776864, R²: -0.09393988248133844, MAE: 0.011799349154214897\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 9.456879342906177e-05\n",
      "Epoch [20/50], Loss: 8.955893281381577e-05\n",
      "Epoch [30/50], Loss: 8.662995969643816e-05\n",
      "Epoch [40/50], Loss: 8.667157089803368e-05\n",
      "Epoch [50/50], Loss: 8.657996659167111e-05\n",
      "Test MSE: 0.0002686207513656815, R²: 0.014227188196150209, MAE: 0.0098318885423377\n",
      "Best model so far: MSE = 0.0002686207513656815, Params = {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.000762031355407089\n",
      "Epoch [20/50], Loss: 0.0002470571198500693\n",
      "Early stopping at epoch 26 with loss 0.0002552704536356032\n",
      "Test MSE: 0.0003120757630079352, R²: -0.1452421334990992, MAE: 0.0117913085359564\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0036026944871991873\n",
      "Epoch [20/100], Loss: 0.0016355653060600162\n",
      "Epoch [30/100], Loss: 0.0005645655910484493\n",
      "Epoch [40/100], Loss: 0.00020952917111571878\n",
      "Epoch [50/100], Loss: 0.00016683420108165592\n",
      "Epoch [60/100], Loss: 0.0001488963607698679\n",
      "Epoch [70/100], Loss: 0.00012259712093509734\n",
      "Epoch [80/100], Loss: 0.00011416673078201711\n",
      "Epoch [90/100], Loss: 0.00011166750482516363\n",
      "Epoch [100/100], Loss: 0.00010636224033078179\n",
      "Test MSE: 0.0005923396349731289, R²: -1.173742365553248, MAE: 0.018142702035198445\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0008494561188854277\n",
      "Epoch [20/100], Loss: 0.00020900345407426357\n",
      "Epoch [30/100], Loss: 8.978295227279887e-05\n",
      "Early stopping at epoch 39 with loss 0.00011373680899851024\n",
      "Test MSE: 0.0002746221697797885, R²: -0.007796557455696407, MAE: 0.010071831238742947\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.004846848547458649\n",
      "Epoch [20/100], Loss: 0.0014399198116734624\n",
      "Epoch [30/100], Loss: 0.0003098454908467829\n",
      "Epoch [40/100], Loss: 0.00024803250562399626\n",
      "Epoch [50/100], Loss: 0.00018588578677736223\n",
      "Epoch [60/100], Loss: 0.00013386683713179082\n",
      "Epoch [70/100], Loss: 0.0001261321158381179\n",
      "Epoch [80/100], Loss: 0.00012029493518639356\n",
      "Epoch [90/100], Loss: 0.00011508240277180448\n",
      "Epoch [100/100], Loss: 0.00011289095709798858\n",
      "Test MSE: 0.000573774243455347, R²: -1.1056118949709166, MAE: 0.016327890663352777\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.00012376059021335095\n",
      "Early stopping at epoch 16 with loss 9.289324952987954e-05\n",
      "Test MSE: 0.0002721404328945296, R²: 0.0013108131964264524, MAE: 0.009764958883299167\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0019376954296603799\n",
      "Epoch [20/100], Loss: 0.0004171082400716841\n",
      "Epoch [30/100], Loss: 0.00016607770521659404\n",
      "Epoch [40/100], Loss: 0.00016078939370345324\n",
      "Epoch [50/100], Loss: 0.00010694144293665886\n",
      "Epoch [60/100], Loss: 0.0001007792234304361\n",
      "Epoch [70/100], Loss: 9.681074880063534e-05\n",
      "Epoch [80/100], Loss: 9.54737261054106e-05\n",
      "Epoch [90/100], Loss: 9.511069947620854e-05\n",
      "Epoch [100/100], Loss: 9.356428199680522e-05\n",
      "Test MSE: 0.0003848938313105646, R²: -0.41246673016880564, MAE: 0.01260800614574811\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 9.635763126425445e-05\n",
      "Epoch [20/100], Loss: 9.006841719383374e-05\n",
      "Early stopping at epoch 23 with loss 8.699992031324655e-05\n",
      "Test MSE: 0.0002673079916329037, R²: 0.019044697068506533, MAE: 0.009749777621117912\n",
      "Best model so far: MSE = 0.0002673079916329037, Params = {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 9.516107820672914e-05\n",
      "Epoch [20/100], Loss: 8.852499740896747e-05\n",
      "Epoch [30/100], Loss: 8.601317676948383e-05\n",
      "Epoch [40/100], Loss: 8.604150934843346e-05\n",
      "Epoch [50/100], Loss: 8.481823169859126e-05\n",
      "Epoch [60/100], Loss: 8.56567858136259e-05\n",
      "Epoch [70/100], Loss: 8.45359027152881e-05\n",
      "Epoch [80/100], Loss: 8.391161827603355e-05\n",
      "Epoch [90/100], Loss: 8.447652362519875e-05\n",
      "Epoch [100/100], Loss: 8.328598050866276e-05\n",
      "Test MSE: 0.0002849825265059423, R²: -0.04581654634080068, MAE: 0.009794536238511193\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0019205240532755852\n",
      "Epoch [20/100], Loss: 0.00018557881412561983\n",
      "Early stopping at epoch 28 with loss 0.0003454429388511926\n",
      "Test MSE: 0.00037017824024474834, R²: -0.3584640907280521, MAE: 0.013727625700417018\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.0062506720423698425\n",
      "Epoch [20/50], Loss: 0.0035277241840958595\n",
      "Epoch [30/50], Loss: 0.0016306432662531734\n",
      "Epoch [40/50], Loss: 0.0006166253006085753\n",
      "Epoch [50/50], Loss: 0.00029498859657905996\n",
      "Test MSE: 0.003474812337394616, R²: -11.751715982139935, MAE: 0.05054013826278508\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.01215669047087431\n",
      "Epoch [20/50], Loss: 0.008138827048242092\n",
      "Epoch [30/50], Loss: 0.004907876718789339\n",
      "Epoch [40/50], Loss: 0.00249349232763052\n",
      "Epoch [50/50], Loss: 0.0009517523576505482\n",
      "Test MSE: 0.0018730551191692907, R²: -5.8736566408212365, MAE: 0.040671753039482\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.0011018752120435238\n",
      "Epoch [20/50], Loss: 0.00019875552970916033\n",
      "Epoch [30/50], Loss: 0.00021260457288008183\n",
      "Early stopping at epoch 31 with loss 0.00020535779185593128\n",
      "Test MSE: 0.0007255456022144407, R²: -1.6625758611373702, MAE: 0.019240043516771246\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.008440718054771423\n",
      "Epoch [20/50], Loss: 0.0034565303940325975\n",
      "Epoch [30/50], Loss: 0.0007345321355387568\n",
      "Epoch [40/50], Loss: 0.00019568708376027644\n",
      "Early stopping at epoch 49 with loss 0.0002351088623981923\n",
      "Test MSE: 0.0004437484913018314, R²: -0.6284490151277586, MAE: 0.013670222846665955\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.00010288116027368233\n",
      "Epoch [20/50], Loss: 9.527509973850101e-05\n",
      "Epoch [30/50], Loss: 8.927489398047328e-05\n",
      "Epoch [40/50], Loss: 8.975640957942232e-05\n",
      "Epoch [50/50], Loss: 8.843650721246377e-05\n",
      "Test MSE: 0.00026867111198968587, R²: 0.014042376956971525, MAE: 0.009703240821757926\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.00012374477228149772\n",
      "Epoch [20/50], Loss: 0.0001586658036103472\n",
      "Early stopping at epoch 22 with loss 0.00015180831542238593\n",
      "Test MSE: 0.0003010037399698679, R²: -0.10461050237193303, MAE: 0.011720612551375171\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.00017083215061575174\n",
      "Epoch [20/50], Loss: 0.00010511705477256328\n",
      "Early stopping at epoch 28 with loss 9.526880603516474e-05\n",
      "Test MSE: 0.00027908765687723996, R²: -0.0241838088119255, MAE: 0.009983313514699967\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.0001547983119962737\n",
      "Early stopping at epoch 17 with loss 0.00013649085303768516\n",
      "Test MSE: 0.00028053260502543503, R²: -0.029486417012205335, MAE: 0.010753691791366728\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.009231878444552422\n",
      "Epoch [20/100], Loss: 0.005731774959713221\n",
      "Epoch [30/100], Loss: 0.0030845983419567347\n",
      "Epoch [40/100], Loss: 0.0013457792811095715\n",
      "Epoch [50/100], Loss: 0.0004982200916856527\n",
      "Epoch [60/100], Loss: 0.00029670068761333823\n",
      "Epoch [70/100], Loss: 0.0002616821730043739\n",
      "Epoch [80/100], Loss: 0.0002267188101541251\n",
      "Epoch [90/100], Loss: 0.00019938738842029124\n",
      "Epoch [100/100], Loss: 0.00018847030878532678\n",
      "Test MSE: 0.001597577559082863, R²: -4.862721009025682, MAE: 0.030700315950128444\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 8.934833022067323e-05\n",
      "Epoch [20/100], Loss: 8.87981805135496e-05\n",
      "Epoch [30/100], Loss: 8.835089829517528e-05\n",
      "Early stopping at epoch 36 with loss 8.721381891518831e-05\n",
      "Test MSE: 0.00026679583021945396, R²: 0.020924204865528928, MAE: 0.009696127910147245\n",
      "Best model so far: MSE = 0.00026679583021945396, Params = {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.00013701901480089873\n",
      "Epoch [20/100], Loss: 0.00010418592137284577\n",
      "Epoch [30/100], Loss: 9.623471851227805e-05\n",
      "Epoch [40/100], Loss: 9.115710417972878e-05\n",
      "Epoch [50/100], Loss: 8.997108670882881e-05\n",
      "Epoch [60/100], Loss: 8.836181950755417e-05\n",
      "Epoch [70/100], Loss: 8.947581954998896e-05\n",
      "Epoch [80/100], Loss: 8.869289013091475e-05\n",
      "Epoch [90/100], Loss: 8.634942059870809e-05\n",
      "Epoch [100/100], Loss: 8.545999298803508e-05\n",
      "Test MSE: 0.00026953705527209813, R²: 0.010864575763183226, MAE: 0.009700261826865958\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0004960730439051986\n",
      "Epoch [20/100], Loss: 0.0001118612926802598\n",
      "Early stopping at epoch 28 with loss 0.0001777410216163844\n",
      "Test MSE: 0.00030592005056734957, R²: -0.12265216630422948, MAE: 0.012106638093984325\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0006818677065894008\n",
      "Epoch [20/100], Loss: 0.0001620464609004557\n",
      "Epoch [30/100], Loss: 0.0001683872687863186\n",
      "Early stopping at epoch 31 with loss 0.00016451011470053345\n",
      "Test MSE: 0.0006938490808810143, R²: -1.5462573384603302, MAE: 0.018140285749038084\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0015668608248233795\n",
      "Epoch [20/100], Loss: 0.0002603965112939477\n",
      "Epoch [30/100], Loss: 0.0001361769245704636\n",
      "Early stopping at epoch 35 with loss 0.00016244023572653532\n",
      "Test MSE: 0.00029079389087019534, R²: -0.06714283986295588, MAE: 0.010764005236731843\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0004631901974789798\n",
      "Epoch [20/100], Loss: 0.00031991791911423206\n",
      "Early stopping at epoch 24 with loss 0.00024973019026219845\n",
      "Test MSE: 0.000646919062078263, R²: -1.3740355858295463, MAE: 0.018728918819679335\n",
      "Training with params: {'batch_size': 32, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0008574294042773545\n",
      "Epoch [20/100], Loss: 0.00028932635905221105\n",
      "Early stopping at epoch 26 with loss 0.0002979807904921472\n",
      "Test MSE: 0.0003483036750640575, R²: -0.27818975780504807, MAE: 0.012822750251032454\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.005805798806250095\n",
      "Epoch [20/50], Loss: 0.0031705659348517656\n",
      "Epoch [30/50], Loss: 0.0014675124548375607\n",
      "Epoch [40/50], Loss: 0.0005840196972712874\n",
      "Epoch [50/50], Loss: 0.0002698799653444439\n",
      "Test MSE: 0.0020474794723179994, R²: -6.513751585743789, MAE: 0.03909439476464328\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.012251174077391624\n",
      "Epoch [20/50], Loss: 0.008628690615296364\n",
      "Epoch [30/50], Loss: 0.005574786104261875\n",
      "Epoch [40/50], Loss: 0.0031066802330315113\n",
      "Epoch [50/50], Loss: 0.0013131560990586877\n",
      "Test MSE: 0.0024050271015058906, R²: -7.825864406463779, MAE: 0.046443752646202874\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.000645375985186547\n",
      "Epoch [20/50], Loss: 0.00017989792104344815\n",
      "Early stopping at epoch 28 with loss 0.00019996085029561073\n",
      "Test MSE: 0.000561066802560779, R²: -1.0589786781483768, MAE: 0.017421377748054596\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.00022741188877262175\n",
      "Epoch [20/50], Loss: 0.0001586417929502204\n",
      "Early stopping at epoch 25 with loss 0.00016294186934828758\n",
      "Test MSE: 0.0002945023581054099, R²: -0.0807520124803136, MAE: 0.011395519270806445\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 9.127104567596689e-05\n",
      "Epoch [20/50], Loss: 8.89272487256676e-05\n",
      "Epoch [30/50], Loss: 8.856541535351425e-05\n",
      "Epoch [40/50], Loss: 8.701199840288609e-05\n",
      "Epoch [50/50], Loss: 8.630399679532275e-05\n",
      "Test MSE: 0.00027538277724657744, R²: -0.0105878018305392, MAE: 0.009811472840492202\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.0001073219464160502\n",
      "Early stopping at epoch 17 with loss 0.00010601433314150199\n",
      "Test MSE: 0.00027251467667516123, R²: -6.257043852264665e-05, MAE: 0.01012900751044943\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.0001550743472762406\n",
      "Early stopping at epoch 18 with loss 0.0001318128197453916\n",
      "Test MSE: 0.000273237414359426, R²: -0.0027148419238496313, MAE: 0.010373008935664531\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.0019425167702138424\n",
      "Epoch [20/50], Loss: 0.0001549699081806466\n",
      "Early stopping at epoch 29 with loss 0.00030246571986936033\n",
      "Test MSE: 0.00035501508473809174, R²: -0.30281899866564643, MAE: 0.013053915957211507\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.008036079816520214\n",
      "Epoch [20/100], Loss: 0.004902437329292297\n",
      "Epoch [30/100], Loss: 0.0026444201357662678\n",
      "Epoch [40/100], Loss: 0.0012215391034260392\n",
      "Epoch [50/100], Loss: 0.0005250918329693377\n",
      "Epoch [60/100], Loss: 0.00029140186961740255\n",
      "Epoch [70/100], Loss: 0.0002215999411419034\n",
      "Epoch [80/100], Loss: 0.0001880558265838772\n",
      "Epoch [90/100], Loss: 0.0001502627128502354\n",
      "Epoch [100/100], Loss: 0.00014080747496336699\n",
      "Test MSE: 0.002014713942575208, R²: -6.393510062255441, MAE: 0.03559569490498019\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.003990039695054293\n",
      "Epoch [20/100], Loss: 0.001940593821927905\n",
      "Epoch [30/100], Loss: 0.0006835713284090161\n",
      "Epoch [40/100], Loss: 0.00017823597590904683\n",
      "Epoch [50/100], Loss: 0.00011761044152081013\n",
      "Early stopping at epoch 59 with loss 0.00012039019202347845\n",
      "Test MSE: 0.00032210264417617665, R²: -0.18203834820920095, MAE: 0.012113310011062249\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0009122347109951079\n",
      "Epoch [20/100], Loss: 0.00016215836512856185\n",
      "Epoch [30/100], Loss: 0.00018219361663796008\n",
      "Early stopping at epoch 31 with loss 0.00017380063945893198\n",
      "Test MSE: 0.0005704148021422932, R²: -1.0932835625825126, MAE: 0.017333239597544604\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0025007566437125206\n",
      "Epoch [20/100], Loss: 0.0004616853839252144\n",
      "Epoch [30/100], Loss: 0.00012178213364677504\n",
      "Early stopping at epoch 38 with loss 0.00019173690816387534\n",
      "Test MSE: 0.00031267546634569665, R²: -0.14744289886256334, MAE: 0.011427919311272305\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0041579753160476685\n",
      "Epoch [20/100], Loss: 0.0012068323558196425\n",
      "Epoch [30/100], Loss: 0.0002383509709034115\n",
      "Epoch [40/100], Loss: 0.00024176813894882798\n",
      "Early stopping at epoch 43 with loss 0.00022302850265987217\n",
      "Test MSE: 0.0011234231957657844, R²: -3.1226898402503265, MAE: 0.024647395141952192\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0014395051402971148\n",
      "Epoch [20/100], Loss: 0.00017997968825511634\n",
      "Epoch [30/100], Loss: 0.00016862613847479224\n",
      "Early stopping at epoch 34 with loss 0.00017938828386832029\n",
      "Test MSE: 0.00030051963489668233, R²: -0.1028339545185879, MAE: 0.011073406467851791\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0017877977807074785\n",
      "Epoch [20/100], Loss: 0.0002516022068448365\n",
      "Early stopping at epoch 29 with loss 0.00030911038629710674\n",
      "Test MSE: 0.0009303337382557, R²: -2.414098502865982, MAE: 0.022468115890642255\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.1, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.00012095525016775355\n",
      "Early stopping at epoch 18 with loss 0.00014855981862638146\n",
      "Test MSE: 0.00029346205660360524, R²: -0.07693435903640045, MAE: 0.010569234868232578\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.0006252717575989664\n",
      "Epoch [20/50], Loss: 0.0001433584839105606\n",
      "Epoch [30/50], Loss: 0.00012517417781054974\n",
      "Early stopping at epoch 34 with loss 0.00012588148820213974\n",
      "Test MSE: 0.00031824780055228616, R²: -0.16789200985348085, MAE: 0.011918388488737061\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.003278070827946067\n",
      "Epoch [20/50], Loss: 0.0014538532122969627\n",
      "Epoch [30/50], Loss: 0.00046835531247779727\n",
      "Epoch [40/50], Loss: 0.0001340616581728682\n",
      "Epoch [50/50], Loss: 0.00011523575085448101\n",
      "Test MSE: 0.0003269579854483137, R²: -0.19985626954911306, MAE: 0.010927267776374481\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.003303609322756529\n",
      "Epoch [20/50], Loss: 0.0007299966528080404\n",
      "Epoch [30/50], Loss: 0.00030333202448673546\n",
      "Epoch [40/50], Loss: 0.00024815721553750336\n",
      "Epoch [50/50], Loss: 0.00014737737365067005\n",
      "Test MSE: 0.0010791245499185268, R²: -2.9601245862483867, MAE: 0.024491606555119512\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.00012902497837785631\n",
      "Epoch [20/50], Loss: 0.00017134459631051868\n",
      "Early stopping at epoch 22 with loss 0.00016332823724951595\n",
      "Test MSE: 0.00031573045622047575, R²: -0.1586539685345172, MAE: 0.011305886158486047\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.003373656654730439\n",
      "Epoch [20/50], Loss: 0.000938396027777344\n",
      "Epoch [30/50], Loss: 0.0002592054952401668\n",
      "Epoch [40/50], Loss: 0.0002285290538566187\n",
      "Epoch [50/50], Loss: 0.0001372492260998115\n",
      "Test MSE: 0.0009561760450012695, R²: -2.508933482124149, MAE: 0.02412311907625828\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 0.001182977226562798\n",
      "Epoch [20/50], Loss: 0.00013348065840546042\n",
      "Epoch [30/50], Loss: 0.00018580300093162805\n",
      "Early stopping at epoch 32 with loss 0.00018209847621619701\n",
      "Test MSE: 0.00030954068987312447, R²: -0.1359390317859006, MAE: 0.011348475734978344\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/50], Loss: 0.00014772932627238333\n",
      "Early stopping at epoch 20 with loss 0.00021116199786774814\n",
      "Test MSE: 0.000397178539330352, R²: -0.45754862017651243, MAE: 0.014322777869557251\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/50], Loss: 8.989993511931971e-05\n",
      "Epoch [20/50], Loss: 8.856017666403204e-05\n",
      "Epoch [30/50], Loss: 8.688029629411176e-05\n",
      "Early stopping at epoch 39 with loss 8.675434219185263e-05\n",
      "Test MSE: 0.0002666331787010932, R²: 0.021521096370684223, MAE: 0.009708175057962757\n",
      "Best model so far: MSE = 0.0002666331787010932, Params = {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.006785737816244364\n",
      "Epoch [20/100], Loss: 0.0036959873978048563\n",
      "Epoch [30/100], Loss: 0.0017172126099467278\n",
      "Epoch [40/100], Loss: 0.0007010383997112513\n",
      "Epoch [50/100], Loss: 0.0003707204305101186\n",
      "Epoch [60/100], Loss: 0.0002626042696647346\n",
      "Epoch [70/100], Loss: 0.00021341121464502066\n",
      "Epoch [80/100], Loss: 0.00016509887063875794\n",
      "Epoch [90/100], Loss: 0.00014976604143157601\n",
      "Epoch [100/100], Loss: 0.00014149403432384133\n",
      "Test MSE: 0.000925672398500511, R²: -2.39699251989972, MAE: 0.02379777368866047\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.010237602517008781\n",
      "Epoch [20/100], Loss: 0.006684555672109127\n",
      "Epoch [30/100], Loss: 0.0038171657361090183\n",
      "Epoch [40/100], Loss: 0.001743142493069172\n",
      "Epoch [50/100], Loss: 0.000570836418773979\n",
      "Epoch [60/100], Loss: 0.00020638146088458598\n",
      "Epoch [70/100], Loss: 0.00016825212514959276\n",
      "Epoch [80/100], Loss: 0.00014310314145404845\n",
      "Epoch [90/100], Loss: 0.0001237828837474808\n",
      "Epoch [100/100], Loss: 0.00012282714305911213\n",
      "Test MSE: 0.00036272167112078496, R²: -0.33110029595654256, MAE: 0.011928998565049462\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0023903725668787956\n",
      "Epoch [20/100], Loss: 0.0004360402817837894\n",
      "Epoch [30/100], Loss: 0.0002470418985467404\n",
      "Early stopping at epoch 36 with loss 0.00024759338703006506\n",
      "Test MSE: 0.0010672493548363044, R²: -2.9165455091018653, MAE: 0.02384697963293637\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 64, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.0033689062111079693\n",
      "Epoch [20/100], Loss: 0.0007921820506453514\n",
      "Epoch [30/100], Loss: 0.0001206555898534134\n",
      "Early stopping at epoch 39 with loss 0.0002194540575146675\n",
      "Test MSE: 0.00031966327835067474, R²: -0.1730864690390408, MAE: 0.011273610511809614\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 0.0013843018095940351\n",
      "Epoch [20/100], Loss: 0.00025051794364117086\n",
      "Epoch [30/100], Loss: 0.0001932476443471387\n",
      "Epoch [40/100], Loss: 0.00013161939568817616\n",
      "Epoch [50/100], Loss: 0.00010429212125018239\n",
      "Epoch [60/100], Loss: 0.0001044226810336113\n",
      "Epoch [70/100], Loss: 9.701021917862818e-05\n",
      "Epoch [80/100], Loss: 9.757268708199263e-05\n",
      "Early stopping at epoch 83 with loss 9.989785758079961e-05\n",
      "Test MSE: 0.0004217551454721816, R²: -0.5477387861181089, MAE: 0.01350622139063769\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0003, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.002333129057660699\n",
      "Epoch [20/100], Loss: 0.0004569982411339879\n",
      "Epoch [30/100], Loss: 0.0001248520566150546\n",
      "Early stopping at epoch 38 with loss 0.00018470782379154116\n",
      "Test MSE: 0.00031289397715557153, R²: -0.14824478037903122, MAE: 0.01103784695979166\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 2}\n",
      "Epoch [10/100], Loss: 9.100062015932053e-05\n",
      "Epoch [20/100], Loss: 8.828476711641997e-05\n",
      "Epoch [30/100], Loss: 8.602684101788327e-05\n",
      "Epoch [40/100], Loss: 8.522660937160254e-05\n",
      "Epoch [50/100], Loss: 8.552963117836043e-05\n",
      "Early stopping at epoch 51 with loss 8.589505887357518e-05\n",
      "Test MSE: 0.00027287823476573317, R²: -0.0013967401903942633, MAE: 0.009773983243883854\n",
      "Training with params: {'batch_size': 64, 'dropout': 0.2, 'epochs': 100, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n",
      "Epoch [10/100], Loss: 0.00012128355592722073\n",
      "Epoch [20/100], Loss: 9.944702469510958e-05\n",
      "Epoch [30/100], Loss: 9.142819180851802e-05\n",
      "Early stopping at epoch 34 with loss 8.733155118534341e-05\n",
      "Test MSE: 0.0002685598691534881, R²: 0.014450611104460775, MAE: 0.009695133712795699\n",
      "Best Hyperparameters: {'batch_size': 64, 'dropout': 0.2, 'epochs': 50, 'hidden_dim': 128, 'learning_rate': 0.0005, 'num_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "\n",
    "# Define a grid of hyperparameters to tune\n",
    "param_grid = {\n",
    "    'hidden_dim': [64, 128],  # Number of hidden units\n",
    "    'num_layers': [2, 3],     # Number of LSTM layers\n",
    "    'dropout': [0.1, 0.2],    # Dropout rate\n",
    "    'learning_rate': [0.0003, 0.0005],  # Learning rate\n",
    "    'epochs': [50, 100],      # Number of epochs\n",
    "    'batch_size': [32, 64]    # Batch size\n",
    "}\n",
    "\n",
    "# Initialize best metrics\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Grid search loop\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Training with params: {params}\")\n",
    "\n",
    "    hidden_dim = params['hidden_dim']\n",
    "    num_layers = params['num_layers']\n",
    "    dropout = params['dropout']\n",
    "    learning_rate = params['learning_rate']\n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train_lstm, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_lstm, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    model = LSTMModel(input_dim=X_train_lstm.shape[2], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train_tensor)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_train_tensor.view(-1, 1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Early stopping condition\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= 10:  # Stop if no improvement\n",
    "            print(f\"Early stopping at epoch {epoch+1} with loss {loss.item()}\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_lstm = model(X_test_tensor)\n",
    "        y_pred_numpy = y_pred_lstm.cpu().detach().numpy().flatten()\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        mse_lstm = mean_squared_error(y_test, y_pred_numpy)\n",
    "        r2_lstm = r2_score(y_test, y_pred_numpy)\n",
    "        mae_lstm = mean_absolute_error(y_test, y_pred_numpy)\n",
    "        \n",
    "        print(f\"Test MSE: {mse_lstm}, R²: {r2_lstm}, MAE: {mae_lstm}\")\n",
    "\n",
    "    # Track the best parameters\n",
    "    if mse_lstm < best_mse:\n",
    "        best_mse = mse_lstm\n",
    "        best_params = params\n",
    "        print(f\"Best model so far: MSE = {best_mse}, Params = {best_params}\")\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bea51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
